"""
æ‹“å±•çŸ¥è¯†ç®¡ç†å™¨ - å¤„ç†PDFå°è¯´åˆ†æç»“æœ

åŠŸèƒ½:
- æ‰©å±•åŸæœ‰çŸ¥è¯†ç®¡ç†åŠŸèƒ½ï¼Œä¸“é—¨å¤„ç†æ–‡å­¦åˆ†æç»“æœ
- ç®¡ç†æ–°çš„çŸ¥è¯†ç±»å‹ï¼ˆnovel-techniqueã€classic-paragraphç­‰ï¼‰
- æä¾›PDFæ‰¹é‡å¤„ç†å’Œå¯¼å…¥åŠŸèƒ½
- æ”¯æŒå¤šå±‚åˆ†æï¼ˆåˆ†æ®µâ†’ç« èŠ‚â†’å…¨ä¹¦ï¼‰
"""
import asyncio
import logging
from typing import List, Dict, Optional, Any
from pathlib import Path
import json
from .manager import KnowledgeManager
from .base import KnowledgeEntry
from .pdf_processor import PDFProcessor
from .literary_analyzer import LiteraryAnalyzer
from .chapter_analyzer import ChapterAnalyzer  # æ–°å¢ç« èŠ‚åˆ†æå™¨
from core.workflow_service import WorkflowService

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NovelKnowledgeExtender:
    """å°è¯´çŸ¥è¯†æ‰©å±•ç®¡ç†å™¨ - ä¸“é—¨å¤„ç†PDFå°è¯´åˆ†æå’ŒçŸ¥è¯†æ¡ç›®ç”Ÿæˆ"""

    def __init__(self, workflow_service: WorkflowService):
        # ä½¿ç”¨å¢å¼ºå­˜å‚¨ç³»ç»Ÿæ¥å¤„ç†åˆ†å±‚åˆ†æç»“æœ
        self.km = KnowledgeManager(storage_path="data/knowledge_repo/enhanced/", use_enhanced_storage=True)
        self.pdf_processor = PDFProcessor()
        self.literary_analyzer = LiteraryAnalyzer(workflow_service)
        self.workflow_service = workflow_service  # ä¿å­˜ä»¥æ”¯æŒæ•´ä½“åˆ†æ

    async def process_pdf_and_import(self, pdf_path: str) -> Dict[str, Any]:
        """
        å®Œæ•´å¤„ç†å•ä¸ªPDFæ–‡ä»¶å¹¶å¯¼å…¥çŸ¥è¯†åº“ï¼ˆæ”¯æŒå¤šå±‚åˆ†æï¼‰

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            file_path = Path(pdf_path)
            if not file_path.exists():
                raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")

            print(f"ğŸ“š å¼€å§‹å¤„ç†PDFæ–‡ä»¶: {file_path.name}")
            logger.info(f"å¼€å§‹å¤„ç†PDFæ–‡ä»¶: {pdf_path}")

            # 1. PDFå†…å®¹æå–å’Œåˆ†å‰²
            print("ğŸ”„ æ­¥éª¤1: æå–å¹¶åˆ†æ®µPDFå†…å®¹...")
            logger.info("æ­¥éª¤1: æå–å¹¶åˆ†æ®µPDFå†…å®¹...")
            segments = self.pdf_processor.process_pdf(pdf_path)

            total_segments = len(segments)
            print(f"âœ… PDFå†…å®¹å·²åˆ†å‰²ä¸º {total_segments} ä¸ªæ®µè½")
            logger.info(f"PDFå†…å®¹å·²åˆ†å‰²ä¸º {total_segments} ä¸ªæ®µè½")

            # 2. ä½¿ç”¨AIåˆ†æå™¨åˆ†ææ¯ä¸ªæ®µè½
            print(f"ğŸ”„ æ­¥éª¤2: ä½¿ç”¨AIåˆ†æ {total_segments} ä¸ªæ®µè½...")
            logger.info("æ­¥éª¤2: ä½¿ç”¨AIåˆ†ææ¯ä¸ªæ®µè½...")
            knowledge_entries = await self.literary_analyzer.batch_analyze(
                segments,
                file_path.name
            )

            # 3. ç”Ÿæˆç« èŠ‚çº§åˆ†ææ‘˜è¦ï¼ˆåŸºäºæ®µè½åˆ†æç»“æœï¼‰
            print("ğŸ”„ æ­¥éª¤3: ç”Ÿæˆç« èŠ‚çº§åˆ†ææ‘˜è¦...")
            logger.info("æ­¥éª¤3: ç”Ÿæˆç« èŠ‚çº§åˆ†ææ‘˜è¦...")
            chapter_analyzer = ChapterAnalyzer(self.workflow_service.model_client)
            chapter_summaries = await chapter_analyzer.analyze_chapters_from_segments(segments)

            # æ·»åŠ ç« èŠ‚æ‘˜è¦ä½œä¸ºæ–°çš„çŸ¥è¯†æ¡ç›®
            print(f"ğŸ”„ æ­£åœ¨ä¿å­˜ç« èŠ‚åˆ†æç»“æœ...")
            for i, chapter_summary in enumerate(chapter_summaries):
                success = await self.km.add_entry(
                    title=chapter_summary.title,
                    content=chapter_summary.content,
                    tags=chapter_summary.tags,
                    knowledge_type=chapter_summary.knowledge_type,
                    source=chapter_summary.source
                )
                if success:
                    knowledge_entries.append(chapter_summary)

            print(f"âœ… ç« èŠ‚åˆ†ææ‘˜è¦å·²ä¿å­˜ ({len(chapter_summaries)} ä¸ª)")

            # 4. æ•´ä½“ç»“æ„åˆ†æï¼ˆåŸºäºç« èŠ‚æ‘˜è¦ï¼‰
            print("ğŸ”„ æ­¥éª¤4: è¿›è¡Œæ•´ä¹¦ç»“æ„åˆ†æ...")
            logger.info("æ­¥éª¤4: è¿›è¡Œæ•´ä¹¦ç»“æ„åˆ†æ...")
            try:
                overall_analysis = await chapter_analyzer.analyze_overall_narrative_structure(
                    segments, file_path.name
                )

                if overall_analysis:
                    success = await self.km.add_entry(
                        title=overall_analysis.title,
                        content=overall_analysis.content,
                        tags=overall_analysis.tags,
                        knowledge_type=overall_analysis.knowledge_type,
                        source=overall_analysis.source
                    )
                    if success:
                        knowledge_entries.append(overall_analysis)
                    print("âœ… æ•´ä¹¦ç»“æ„åˆ†æå·²ä¿å­˜")
            except Exception as e:
                logger.warning(f"æ•´ä¹¦ç»“æ„åˆ†æå‡ºé”™ï¼Œå°†è·³è¿‡: {str(e)}")
                print(f"âš ï¸  æ•´ä¹¦ç»“æ„åˆ†æå‡ºé”™: {str(e)}")

            # 5. æ€»ç»“å¤„ç†ç»“æœ
            print(f"ğŸ”„ æ­¥éª¤5: ä¿å­˜æ‰€æœ‰çŸ¥è¯†æ¡ç›®åˆ°çŸ¥è¯†åº“...")
            logger.info(f"æ­¥éª¤5: æ‰¹é‡ä¿å­˜ {len(knowledge_entries)} ä¸ªçŸ¥è¯†æ¡ç›®åˆ°çŸ¥è¯†åº“...")
            successful_imports = 0
            total_entries = len(knowledge_entries)
            for i, entry in enumerate(knowledge_entries):
                try:
                    success = await self.km.add_entry(
                        title=entry.title,
                        content=entry.content,
                        tags=entry.tags,
                        knowledge_type=entry.knowledge_type,
                        source=entry.source
                    )
                    if success:
                        successful_imports += 1
                except Exception as e:
                    logger.error(f"ä¿å­˜çŸ¥è¯†æ¡ç›®å¤±è´¥ (ID: {entry.id}): {str(e)}")

                # æ¯å¤„ç†10ä¸ªæ¡ç›®æˆ–åœ¨å…³é”®èŠ‚ç‚¹æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if (i + 1) % 10 == 0 or (i + 1) == total_entries or i == 0:
                    progress_percent = ((i + 1) / total_entries) * 100
                    print(f"ğŸ’¾ å·²ä¿å­˜ {i+1}/{total_entries} ä¸ªçŸ¥è¯†æ¡ç›®åˆ°JSONæ–‡ä»¶ ({progress_percent:.1f}%)")

                # æ¯å¤„ç†50ä¸ªæ¡ç›®åï¼Œæ˜¾ç¤ºå½“å‰å­˜å‚¨çŠ¶æ€
                if (i + 1) % 50 == 0:
                    current_stats = await self.get_novel_analysis_stats()
                    print(f"ğŸ“Š å½“å‰çŸ¥è¯†åº“ç»Ÿè®¡: å…± {current_stats['total_knowledge_entries']} ä¸ªæ¡ç›® | "
                          f"å°è¯´åˆ†æ: {current_stats['novel_analysis_entries']} ä¸ª | "
                          f"æ¥æºPDF: {current_stats['unique_sources']} ä¸ª")

            # 6. ç”Ÿæˆå¤„ç†æŠ¥å‘Š
            result = {
                'status': 'success',
                'pdf_file': pdf_path,
                'total_segments_analyzed': total_segments,
                'knowledge_entries_created': len(knowledge_entries),
                'successful_imports': successful_imports,
                'failed_imports': len(knowledge_entries) - successful_imports,
                'message': f"æˆåŠŸå¤„ç†PDFæ–‡ä»¶ï¼Œåˆ›å»ºäº† {successful_imports} ä¸ªçŸ¥è¯†æ¡ç›® (å«æ®µè½çº§ã€ç« èŠ‚çº§ã€å…¨ä¹¦çº§åˆ†æ)"
            }

            print(f"ğŸ‰ PDFå¤„ç†å®Œæˆ!")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   - æ€»æ®µè½æ•°: {total_segments}")
            print(f"   - åˆ›å»ºçŸ¥è¯†æ¡ç›®: {len(knowledge_entries)}")
            print(f"   - æˆåŠŸå¯¼å…¥: {successful_imports}")
            print(f"   - å¤±è´¥å¯¼å…¥: {len(knowledge_entries) - successful_imports}")

            logger.info(f"PDFå¤„ç†å®Œæˆ: {result['message']}")
            return result

        except Exception as e:
            logger.error(f"å¤„ç†PDFæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            print(f"âŒ å¤„ç†PDFæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return {
                'status': 'error',
                'pdf_file': pdf_path,
                'message': str(e),
                'total_segments_analyzed': 0,
                'knowledge_entries_created': 0,
                'successful_imports': 0,
                'failed_imports': 0
            }

    async def process_pdf_batch(self, pdf_paths: List[str]) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªPDFæ–‡ä»¶

        Args:
            pdf_paths: PDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            æ‰¹é‡å¤„ç†ç»“æœæ±‡æ€»
        """
        total_files = len(pdf_paths)
        print(f"ğŸ“š å¼€å§‹æ‰¹é‡å¤„ç† {total_files} ä¸ªPDFæ–‡ä»¶...")
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(pdf_paths)} ä¸ªPDFæ–‡ä»¶...")

        results = []
        total_successful = 0
        total_entries = 0

        for i, pdf_path in enumerate(pdf_paths):
            current_file_num = i + 1
            progress_percent = current_file_num / total_files * 100
            print(f"\nğŸ“‹ è¿›åº¦: {current_file_num}/{total_files} ({progress_percent:.1f}%) - {Path(pdf_path).name}")
            logger.info(f"å¤„ç†è¿›åº¦: {current_file_num}/{total_files} - {pdf_path}")

            result = await self.process_pdf_and_import(pdf_path)
            results.append(result)

            total_successful += result.get('successful_imports', 0)
            total_entries += result.get('knowledge_entries_created', 0)

            # æ˜¾ç¤ºå½“å‰è¿›åº¦æ±‡æ€»
            print(f"ğŸ“ˆ å½“å‰æ±‡æ€»: å·²å¤„ç† {current_file_num} ä¸ªæ–‡ä»¶ï¼Œ"
                  f"ç´¯è®¡ç”Ÿæˆ {total_entries} ä¸ªçŸ¥è¯†æ¡ç›®ï¼Œ"
                  f"æˆåŠŸå¯¼å…¥ {total_successful} ä¸ª")

        # æ±‡æ€»ç»“æœ
        overall_result = {
            'status': 'completed',
            'total_files_processed': len(pdf_paths),
            'successful_files': sum(1 for r in results if r.get('status') == 'success'),
            'failed_files': sum(1 for r in results if r.get('status') == 'error'),
            'total_knowledge_entries_created': total_entries,
            'total_successful_imports': total_successful,
            'failed_imports': total_entries - total_successful,
            'individual_results': results
        }

        print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   - æ€»å¤„ç†æ–‡ä»¶æ•°: {total_files}")
        print(f"   - æˆåŠŸå¤„ç†: {overall_result['successful_files']}")
        print(f"   - å¤±è´¥å¤„ç†: {overall_result['failed_files']}")
        print(f"   - åˆ›å»ºçŸ¥è¯†æ¡ç›®: {total_entries}")
        print(f"   - æˆåŠŸå¯¼å…¥: {total_successful}")
        print(f"   - å¯¼å…¥å¤±è´¥: {total_entries - total_successful}")

        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸå¤„ç† {overall_result['successful_files']} ä¸ªæ–‡ä»¶, "
                   f"åˆ›å»º {total_successful} ä¸ªçŸ¥è¯†æ¡ç›®")

        return overall_result

    async def process_directory(self, directory_path: str, file_pattern: str = "*.pdf") -> Dict[str, Any]:
        """
        å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶

        Args:
            directory_path: åŒ…å«PDFæ–‡ä»¶çš„ç›®å½•è·¯å¾„
            file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼

        Returns:
            å¤„ç†ç»“æœæ±‡æ€»
        """
        dir_path = Path(directory_path)
        if not dir_path.exists():
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")

        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„PDFæ–‡ä»¶
        pdf_files = list(dir_path.glob(file_pattern))
        pdf_files_str = [str(f) for f in pdf_files if f.suffix.lower() == '.pdf']

        if not pdf_files_str:
            return {
                'status': 'error',
                'message': f"åœ¨ç›®å½• {directory_path} ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶"
            }

        logger.info(f"åœ¨ç›®å½•ä¸­æ‰¾åˆ° {len(pdf_files_str)} ä¸ªPDFæ–‡ä»¶")
        return await self.process_pdf_batch(pdf_files_str)

    async def get_novel_analysis_stats(self) -> Dict[str, Any]:
        """
        è·å–å°è¯´åˆ†æçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯

        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        all_entries = await self.km.get_all_entries()

        # æŒ‰çŸ¥è¯†ç±»å‹ç»Ÿè®¡
        type_counts = {}
        for entry in all_entries:
            k_type = entry.knowledge_type
            if k_type in ['novel-technique', 'classic-paragraph', 'character-development', 'dialogue-technique']:
                type_counts[k_type] = type_counts.get(k_type, 0) + 1

        # æŒ‰æ¥æºç»Ÿè®¡
        source_counts = {}
        for entry in all_entries:
            source = entry.source
            if source.startswith('PDF:'):
                source_counts[source] = source_counts.get(source, 0) + 1

        return {
            'total_knowledge_entries': len(all_entries),
            'novel_analysis_entries': sum(type_counts.values()),
            'breakdown_by_type': type_counts,
            'breakdown_by_source': source_counts,
            'unique_sources': len(source_counts)
        }

    async def search_novel_techniques(self, query: str, tags: List[str] = None) -> List[KnowledgeEntry]:
        """
        æœç´¢å°è¯´æŠ€å·§ç›¸å…³çš„çŸ¥è¯†æ¡ç›®

        Args:
            query: æœç´¢æŸ¥è¯¢è¯
            tags: æ ‡ç­¾ç­›é€‰æ¡ä»¶

        Returns:
            åŒ¹é…çš„çŸ¥è¯†æ¡ç›®åˆ—è¡¨
        """
        # æœç´¢æ‰€æœ‰å°è¯´åˆ†æç±»å‹çš„çŸ¥è¯†å…¥å£
        types_to_search = ['novel-technique', 'classic-paragraph', 'character-development', 'dialogue-technique']

        results = []
        all_entries = await self.km.get_all_entries()

        for entry in all_entries:
            if entry.knowledge_type in types_to_search:
                # æ£€æŸ¥æŸ¥è¯¢åŒ¹é…
                matches_query = (query.lower() in entry.title.lower() or
                               query.lower() in entry.content.lower())

                # æ£€æŸ¥æ ‡ç­¾åŒ¹é…
                if tags is None or all(tag in entry.tags for tag in tags):
                    # å¦‚æœæ»¡è¶³æ‰€æœ‰æ¡ä»¶ï¼ŒåŠ å…¥ç»“æœ
                    if query == "" or matches_query:
                        results.append(entry)

        return results

    async def get_examples_by_novel_type(self, novel_type: str, limit: int = 10) -> List[KnowledgeEntry]:
        """
        è·å–ç‰¹å®šç±»å‹çš„å°è¯´æŠ€å·§ç¤ºä¾‹

        Args:
            novel_type: å°è¯´æŠ€å·§ç±»å‹ (å¦‚ 'character-development', 'dialogue', 'descriptive' ç­‰)
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            åŒ¹é…çš„çŸ¥è¯†æ¡ç›®åˆ—è¡¨
        """
        all_entries = await self.km.get_all_entries()
        matching_entries = []

        type_mappings = {
            'character-development': ['character-development'],
            'dialogue-techniques': ['dialogue-technique'],
            'descriptive-passages': ['novel-technique'],
            'classic-passages': ['classic-paragraph'],
            'narrative-techniques': ['novel-technique']
        }

        if novel_type in type_mappings:
            target_types = type_mappings[novel_type]
            for entry in all_entries:
                if entry.knowledge_type in target_types:
                    matching_entries.append(entry)

        # å¯ä»¥è¿›ä¸€æ­¥æ ¹æ®æ ‡ç­¾è¿‡æ»¤
        if novel_type == 'character-development':
            matching_entries = [e for e in matching_entries if 'character' in str(e.tags).lower()]
        elif novel_type == 'dialogue-techniques':
            matching_entries = [e for e in matching_entries if 'dialogue' in str(e.tags).lower()]
        elif novel_type == 'descriptive-passages':
            matching_entries = [e for e in matching_entries if 'descriptive' in str(e.tags).lower()]

        return matching_entries[:limit]