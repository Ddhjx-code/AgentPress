"""
æ–‡å­¦åˆ†æä»£ç†æ¨¡å—

åŠŸèƒ½:
- ä½¿ç”¨AIæ¨¡å‹åˆ†ææå–çš„å°è¯´ç‰‡æ®µ
- è¯†åˆ«æ–‡å­¦æŠ€å·§ã€èŠ‚å¥æ¨¡å¼ã€ç»å…¸æ®µè½
- ç”Ÿæˆç»“æ„åŒ–çš„çŸ¥è¯†æ¡ç›®
- æä¾›å¯¹æ®µè½å†…å®¹çš„æ·±åº¦è§£è¯»
"""
import logging
from typing import List, Dict, Any, Optional
from .base import KnowledgeEntry
from core.workflow_service import WorkflowService

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LiteraryAnalyzer:
    """æ–‡å­¦åˆ†æä»£ç† - ä½¿ç”¨AIåˆ†æå°è¯´å†…å®¹"""

    def __init__(self, workflow_service: WorkflowService):
        self.workflow_service = workflow_service
        if not workflow_service.model_client:
            raise ValueError("WorkflowService å¿…é¡»å·²åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯")

        # ä½¿ç”¨æ–°çš„æç¤ºè¯æ ¼å¼ - ä½†éœ€è¦å…¼å®¹AIè¿”å›çš„æ ¼å¼
        import os
        from pathlib import Path

        prompts_dir = Path("prompts") if Path("prompts").exists() else Path(__file__).parent.parent / "prompts"

        technique_prompt_path = prompts_dir / "literary_analysis_base.md"
        if technique_prompt_path.exists():
            with open(technique_prompt_path, 'r', encoding='utf-8') as f:
                full_content = f.read()

            # ä¿ç•™åŸæœ‰çš„è¯¦ç»†æç¤ºè¯æ ¼å¼ï¼Œå› ä¸ºJSONæ ¼å¼å¯èƒ½ä¼šå¯¼è‡´AIè¡Œä¸ºä¸ä¸€è‡´
            self.analysis_prompt_templates = {
                "technique_analysis": """è¯·æ·±åº¦åˆ†æä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µçš„æ–‡å­¦æŠ€å·§ï¼š

æ–‡æœ¬å†…å®¹: {text}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œåˆ†æï¼š
1. ä¿®è¾æ‰‹æ³• (å¦‚æ¯”å–»ã€æ‹Ÿäººã€æ’æ¯”ç­‰å½¢è±¡åŒ–è¡¨è¾¾æŠ€å·§)
2. å™è¿°æŠ€å·§ (å¦‚è§†è§’é€‰æ‹©ã€æ—¶é—´å¤„ç†ã€èŠ‚å¥æ§åˆ¶ã€ä¼ç¬”ã€é“ºå«ç­‰)
3. æƒ…æ„Ÿè¡¨è¾¾æ–¹æ³• (å¦‚å¯¹æ¯”ã€è¡¬æ‰˜ã€è±¡å¾ã€æ°›å›´è¥é€ ç­‰)
4. ç»“æ„å¸ƒå±€æŠ€å·§ (å¦‚èµ·æ‰¿è½¬åˆã€å¼€åˆå‘¼åº”ã€å±‚æ¬¡æ¨è¿›ç­‰)
5. äººç‰©å¡‘é€ æŠ€æ³• (å¦‚å¤–è²Œæå†™ã€å¿ƒç†åˆ»ç”»ã€å¯¹è¯è¿ç”¨ã€è¡Œä¸ºå±•ç¤ºç­‰)
6. è¯­è¨€è‰ºæœ¯ç‰¹è‰² (å¦‚è¯å¥é€‰æ‹©ã€è¯­è°ƒèŠ‚å¥ã€é£æ ¼ç‰¹ç‚¹ç­‰)

è¯·ç”¨ä¸­æ–‡ç®€æ´è¾“å‡ºï¼Œæ¯é¡¹ä¸è¶…è¿‡50å­—ã€‚""",
                "rhythm_analysis": """è¯·æ·±åº¦åˆ†æä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µçš„èŠ‚å¥è°ƒæ§æŠ€å·§ï¼š

æ–‡æœ¬å†…å®¹: {text}

è¯·è¯„ä»·ï¼š
1. æƒ…èŠ‚èŠ‚å¥ï¼šèµ·æ‰¿è½¬åˆã€æ‚¬å¿µè®¾ç½®ã€è½¬æŠ˜å®‰æ’ã€æƒ…èŠ‚å¼ åŠ›ç­‰
2. ä¿¡æ¯é€’è¿›èŠ‚å¥ï¼šä¿¡æ¯é‡Šæ”¾çš„é€Ÿåº¦ä¸æ–¹å¼ï¼Œé€æ­¥æ­ç¤ºè¿˜æ˜¯çªç„¶æ­éœ²
3. æƒ…æ„Ÿæ³¢åŠ¨èŠ‚å¥ï¼šæƒ…ç»ªçš„èµ·ä¼å˜åŒ–ä¸æ§åˆ¶ï¼Œé“ºå«ä¸é«˜æ½®è®¾è®¡
4. èŠ‚å¥å˜åŒ–æŠ€å·§ï¼šæ¬²æ‰¬å…ˆæŠ‘ã€å…ˆå£°å¤ºäººã€èŠ‚å¥çªå˜ç­‰æ‰‹æ³•

è¯´æ˜ï¼šé‡ç‚¹å…³æ³¨æ•…äº‹å™è¿°èŠ‚å¥å®‰æ’ã€æƒ…èŠ‚æ¨è¿›æŠ€å·§åŠæƒ…ç»ªæ§åˆ¶æ–¹æ³•ã€‚""",
                "classic_paragraph_analysis": """è¯·åˆ¤æ–­ä»¥ä¸‹æ®µè½æ˜¯å¦å…·æœ‰ç»å…¸æ–‡å­¦æ®µè½çš„ä»·å€¼ï¼š

æ–‡æœ¬å†…å®¹: {text}

åˆ¤æ–­æ ‡å‡†ï¼š
1. æ˜¯å¦è¿ç”¨äº†å“è¶Šçš„æ–‡å­¦æŠ€å·§ï¼ˆå¦‚ä¿®è¾ã€ç»“æ„ã€èŠ‚å¥ç­‰ï¼‰
2. æ˜¯å¦å±•ç°äº†ç‹¬ç‰¹çš„è‰ºæœ¯æ‰‹æ³•æˆ–åˆ›æ–°æ€§è¡¨è¾¾
3. æ˜¯å¦ä½“ç°äº†æ·±åˆ»çš„äººæ–‡æ€æƒ³æˆ–å“²ç†å†…æ¶µ
4. æ˜¯å¦å…·æœ‰ä»£è¡¨æ€§æˆ–å…¸å‹æ€§çš„æ–‡å­¦æ„ä¹‰

è¯·ç»™å‡º"æ˜¯"æˆ–"å¦"çš„åˆ¤æ–­ï¼Œå¹¶ç®€è¦è¯´æ˜ç†ç”±ã€‚""",
                "character_development_analysis": """è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µä¸­çš„äººç‰©å¡‘é€ è‰ºæœ¯ï¼š

æ–‡æœ¬å†…å®¹: {text}

åˆ†æè¦ç‚¹ï¼š
1. äººç‰©æ€§æ ¼åˆ»ç”»æŠ€æ³• (å¦‚å¤–è²Œã€è¯­è¨€ã€è¡Œä¸ºã€å¿ƒç†ç­‰å¡‘é€ æ–¹æ³•)
2. äººç‰©å½¢è±¡çš„ç«‹ä½“æ€§ä¸ç‹¬ç‰¹æ€§
3. äººç‰©ä¸æƒ…èŠ‚çš„å…³ç³»åŠæ¨åŠ¨ä½œç”¨
4. äººç‰©å¡‘é€ çš„æŠ€å·§åˆ›æ–°ç‚¹
5. äººç‰©è±¡å¾æ„ä¹‰æˆ–å…¸å‹æ€§

å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰æ˜æ˜¾äººç‰©ï¼Œè¯·è¯´æ˜ã€‚""",
                "dialogue_analysis": """è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µä¸­å¯¹è¯çš„æ–‡å­¦æŠ€å·§ï¼š

æ–‡æœ¬å†…å®¹: {text}

åˆ†æé‡ç‚¹ï¼š
1. å¯¹è¯çš„æ€§æ ¼åŒ–ç‰¹å¾ (ä½“ç°äººç‰©ç‹¬ç‰¹è¯­è¨€é£æ ¼)
2. å¯¹è¯çš„æˆå‰§æ€§ä½œç”¨ (æ¨è¿›æƒ…èŠ‚ã€åˆ¶é€ å†²çªã€æ­ç¤ºçŸ›ç›¾)
3. å¯¹è¯çš„æ½œå°è¯æŠ€å·§ (è¨€å¤–ä¹‹æ„ã€å¼¦å¤–ä¹‹éŸ³)
4. å¯¹è¯ä¸å™è¿°çš„é…åˆ (å¯¹è¯ä¸æ—ç™½çš„å¹³è¡¡å¤„ç†)
5. å¯¹è¯çš„èŠ‚å¥æ§åˆ¶ (å¿«æ…¢ã€åœé¡¿ã€äº¤äº’æ¨¡å¼)

å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰å¯¹è¯ï¼Œè¯·è¯´æ˜ã€‚"""
            }
        else:
            # å¦‚æœæç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å…¼å®¹çš„æ¨¡æ¿
            self.analysis_prompt_templates = {
                "technique_analysis": """è¯·æ·±åº¦åˆ†æä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µçš„æ–‡å­¦æŠ€å·§ï¼š

æ–‡æœ¬å†…å®¹: {text}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œåˆ†æï¼š
1. ä¿®è¾æ‰‹æ³• (å¦‚æ¯”å–»ã€æ‹Ÿäººã€æ’æ¯”ç­‰å½¢è±¡åŒ–è¡¨è¾¾æŠ€å·§)
2. å™è¿°æŠ€å·§ (å¦‚è§†è§’é€‰æ‹©ã€æ—¶é—´å¤„ç†ã€èŠ‚å¥æ§åˆ¶ã€ä¼ç¬”ã€é“ºå«ç­‰)
3. æƒ…æ„Ÿè¡¨è¾¾æ–¹æ³• (å¦‚å¯¹æ¯”ã€è¡¬æ‰˜ã€è±¡å¾ã€æ°›å›´è¥é€ ç­‰)
4. ç»“æ„å¸ƒå±€æŠ€å·§ (å¦‚èµ·æ‰¿è½¬åˆã€å¼€åˆå‘¼åº”ã€å±‚æ¬¡æ¨è¿›ç­‰)
5. äººç‰©å¡‘é€ æŠ€æ³• (å¦‚å¤–è²Œæå†™ã€å¿ƒç†åˆ»ç”»ã€å¯¹è¯è¿ç”¨ã€è¡Œä¸ºå±•ç¤ºç­‰)
6. è¯­è¨€è‰ºæœ¯ç‰¹è‰² (å¦‚è¯å¥é€‰æ‹©ã€è¯­è°ƒèŠ‚å¥ã€é£æ ¼ç‰¹ç‚¹ç­‰)

è¯·ç”¨ä¸­æ–‡ç®€æ´è¾“å‡ºï¼Œæ¯é¡¹ä¸è¶…è¿‡50å­—ã€‚""",
                "rhythm_analysis": """è¯·æ·±åº¦åˆ†æä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µçš„èŠ‚å¥è°ƒæ§æŠ€å·§ï¼š

æ–‡æœ¬å†…å®¹: {text}

è¯·è¯„ä»·ï¼š
1. æƒ…èŠ‚èŠ‚å¥ï¼šèµ·æ‰¿è½¬åˆã€æ‚¬å¿µè®¾ç½®ã€è½¬æŠ˜å®‰æ’ã€æƒ…èŠ‚å¼ åŠ›ç­‰
2. ä¿¡æ¯é€’è¿›èŠ‚å¥ï¼šä¿¡æ¯é‡Šæ”¾çš„é€Ÿåº¦ä¸æ–¹å¼ï¼Œé€æ­¥æ­ç¤ºè¿˜æ˜¯çªç„¶æ­éœ²
3. æƒ…æ„Ÿæ³¢åŠ¨èŠ‚å¥ï¼šæƒ…ç»ªçš„èµ·ä¼å˜åŒ–ä¸æ§åˆ¶ï¼Œé“ºå«ä¸é«˜æ½®è®¾è®¡
4. èŠ‚å¥å˜åŒ–æŠ€å·§ï¼šæ¬²æ‰¬å…ˆæŠ‘ã€å…ˆå£°å¤ºäººã€èŠ‚å¥çªå˜ç­‰æ‰‹æ³•

è¯´æ˜ï¼šé‡ç‚¹å…³æ³¨æ•…äº‹å™è¿°èŠ‚å¥å®‰æ’ã€æƒ…èŠ‚æ¨è¿›æŠ€å·§åŠæƒ…ç»ªæ§åˆ¶æ–¹æ³•ã€‚""",
                "classic_paragraph_analysis": """è¯·åˆ¤æ–­ä»¥ä¸‹æ®µè½æ˜¯å¦å…·æœ‰ç»å…¸æ–‡å­¦æ®µè½çš„ä»·å€¼ï¼š

æ–‡æœ¬å†…å®¹: {text}

åˆ¤æ–­æ ‡å‡†ï¼š
1. æ˜¯å¦è¿ç”¨äº†å“è¶Šçš„æ–‡å­¦æŠ€å·§ï¼ˆå¦‚ä¿®è¾ã€ç»“æ„ã€èŠ‚å¥ç­‰ï¼‰
2. æ˜¯å¦å±•ç°äº†ç‹¬ç‰¹çš„è‰ºæœ¯æ‰‹æ³•æˆ–åˆ›æ–°æ€§è¡¨è¾¾
3. æ˜¯å¦ä½“ç°äº†æ·±åˆ»çš„äººæ–‡æ€æƒ³æˆ–å“²ç†å†…æ¶µ
4. æ˜¯å¦å…·æœ‰ä»£è¡¨æ€§æˆ–å…¸å‹æ€§çš„æ–‡å­¦æ„ä¹‰

è¯·ç»™å‡º"æ˜¯"æˆ–"å¦"çš„åˆ¤æ–­ï¼Œå¹¶ç®€è¦è¯´æ˜ç†ç”±ã€‚""",
                "character_development_analysis": """è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µä¸­çš„äººç‰©å¡‘é€ è‰ºæœ¯ï¼š

æ–‡æœ¬å†…å®¹: {text}

åˆ†æè¦ç‚¹ï¼š
1. äººç‰©æ€§æ ¼åˆ»ç”»æŠ€æ³• (å¦‚å¤–è²Œã€è¯­è¨€ã€è¡Œä¸ºã€å¿ƒç†ç­‰å¡‘é€ æ–¹æ³•)
2. äººç‰©å½¢è±¡çš„ç«‹ä½“æ€§ä¸ç‹¬ç‰¹æ€§
3. äººç‰©ä¸æƒ…èŠ‚çš„å…³ç³»åŠæ¨åŠ¨ä½œç”¨
4. äººç‰©å¡‘é€ çš„æŠ€å·§åˆ›æ–°ç‚¹
5. äººç‰©è±¡å¾æ„ä¹‰æˆ–å…¸å‹æ€§

å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰æ˜æ˜¾äººç‰©ï¼Œè¯·è¯´æ˜ã€‚""",
                "dialogue_analysis": """è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µä¸­å¯¹è¯çš„æ–‡å­¦æŠ€å·§ï¼š

æ–‡æœ¬å†…å®¹: {text}

åˆ†æé‡ç‚¹ï¼š
1. å¯¹è¯çš„æ€§æ ¼åŒ–ç‰¹å¾ (ä½“ç°äººç‰©ç‹¬ç‰¹è¯­è¨€é£æ ¼)
2. å¯¹è¯çš„æˆå‰§æ€§ä½œç”¨ (æ¨è¿›æƒ…èŠ‚ã€åˆ¶é€ å†²çªã€æ­ç¤ºçŸ›ç›¾)
3. å¯¹è¯çš„æ½œå°è¯æŠ€å·§ (è¨€å¤–ä¹‹æ„ã€å¼¦å¤–ä¹‹éŸ³)
4. å¯¹è¯ä¸å™è¿°çš„é…åˆ (å¯¹è¯ä¸æ—ç™½çš„å¹³è¡¡å¤„ç†)
5. å¯¹è¯çš„èŠ‚å¥æ§åˆ¶ (å¿«æ…¢ã€åœé¡¿ã€äº¤äº’æ¨¡å¼)

å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰å¯¹è¯ï¼Œè¯·è¯´æ˜ã€‚"""
            }

    async def analyze_paragraph(
        self,
        paragraph_data: Dict,
        source_novel: str
    ) -> Optional[KnowledgeEntry]:
        """
        åˆ†æå•ä¸ªæ®µè½å¹¶ç”ŸæˆçŸ¥è¯†æ¡ç›®

        Args:
            paragraph_data: æ®µè½æ•°æ®å­—å…¸
            source_novel: åŸå§‹å°è¯´åç§°

        Returns:
            KnowledgeEntryå¯¹è±¡æˆ–None
        """
        text = paragraph_data['text']
        original_title = paragraph_data.get('original_title', 'Unknown')
        chunk_id = paragraph_data['id']
        chapter_info = paragraph_data.get('chapter_info', {})

        try:
            # å¤šç»´åº¦åˆ†æ
            techniques = await self._analyze_technique(text)
            rhythm = await self._analyze_rhythm(text)
            is_classic, classic_reason = await self._identify_classic_paragraph(text)
            char_dev = await self._analyze_character_development(text)
            dialogue = await self._analyze_dialogue(text)

            # å†³å®šç”Ÿæˆä»€ä¹ˆç±»å‹çš„çŸ¥è¯†æ¡ç›®
            knowledge_entries = []

            # æ ¹æ®åˆ†æç»“æœåˆ›å»ºä¸åŒç±»å‹çš„çŸ¥è¯†æ¡ç›®
            if techniques:
                technique_entry = await self._create_technique_entry(
                    text, techniques, original_title, chunk_id, chapter_info
                )
                knowledge_entries.append(technique_entry)

            if is_classic:
                classic_entry = await self._create_classic_paragraph_entry(
                    text, classic_reason, original_title, chunk_id, chapter_info
                )
                knowledge_entries.append(classic_entry)

            if char_dev and "æ²¡æœ‰æ˜æ˜¾äººç‰©" not in char_dev:
                char_entry = await self._create_character_entry(
                    text, char_dev, original_title, chunk_id, chapter_info
                )
                knowledge_entries.append(char_entry)

            # è¿”å›æœ€æœ‰ä»·å€¼çš„ä¸€ä¸ªæ¡ç›®
            # ä¼˜å…ˆè¿”å›ç»å…¸æ®µè½ï¼Œå…¶æ¬¡æŠ€å·§æ¡ç›®
            for entry in knowledge_entries:
                if entry.knowledge_type == "classic-paragraph":
                    return entry

            # å¦‚æœæœ‰æŠ€å·§æ¡ç›®ï¼Œè¿”å›ç¬¬ä¸€ä¸ª
            for entry in knowledge_entries:
                if entry.knowledge_type == "novel-technique":
                    return entry

            # å¦‚æœæœ‰è§’è‰²æ¡ç›®ï¼Œè¿”å›ç¬¬ä¸€ä¸ª
            for entry in knowledge_entries:
                if entry.knowledge_type == "character-development":
                    return entry

            # å¦‚æœæ®µè½åŒ…å«å¯¹è¯ä¸”åˆ†æç»“æœæœ‰ä»·å€¼ï¼Œåˆ›å»ºå¯¹è¯æŠ€å·§æ¡ç›®
            if "æ²¡æœ‰å¯¹è¯" not in dialogue:
                dialogue_entry = await self._create_dialogue_entry(
                    text, dialogue, original_title, chunk_id, chapter_info
                )
                return dialogue_entry

        except Exception as e:
            logger.error(f"åˆ†ææ®µè½æ—¶å‡ºé”™ (ID: {chunk_id}): {str(e)}")
            return None

        return None

    async def _analyze_technique(self, text: str) -> str:
        """åˆ†ææ–‡æœ¬çš„æ–‡å­¦æŠ€å·§ - ç°åœ¨è¿”å›JSONè§£æåçš„æŠ€å·§ä¿¡æ¯"""
        if len(text.strip()) < 10:
            return ""
        prompt = self.analysis_prompt_templates["technique_analysis"].format(text=text)
        try:
            import json
            response = await self._get_ai_response(prompt)

            # å°è¯•è§£æAIè¿”å›çš„JSONæ ¼å¼æ•°æ®
            try:
                # å°è¯•è§£æJSONå“åº”
                if response.strip().startswith('```json'):
                    # æå–ä»£ç å—ä¸­çš„JSON
                    json_str = response.strip()
                    if json_str.startswith('```json'):
                        json_str = json_str[7:]  # ç§»é™¤```json
                    if json_str.endswith('```'):
                        json_str = json_str[:-3]  # ç§»é™¤æœ«å°¾ ```
                    json_str = json_str.strip()
                    data = json.loads(json_str)
                elif response.strip().startswith('{') and response.strip().endswith('}'):
                    # ç›´æ¥æ˜¯JSON
                    data = json.loads(response)
                else:
                    # ä¸æ˜¯JSONæ ¼å¼ï¼Œè¿”å›åŸå§‹å“åº”
                    return response
            except json.JSONDecodeError:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å“åº”
                return response

            # å°‡å¼åŒ–JSONå“åº”ä¸ºäººå¯è¯»çš„åˆ†æå†…å®¹
            techniques_info = []
            if "techniques" in data and data["techniques"]:
                for tech in data["techniques"]:
                    techniques_info.append(
                        f"æŠ€å·§: {tech.get('name', 'æœªçŸ¥æŠ€å·§')}\n"
                        f"ç±»å‹: {tech.get('module', '?')} \n"
                        f"åº”ç”¨æƒ…å¢ƒ: {tech.get('trigger_context', '')}\n"
                        f"å®ç°æœºåˆ¶: {tech.get('mechanism', '')}\n"
                        f"æ•ˆæœ: {tech.get('effect', '')}\n"
                        f"å¸¸è§ç»„åˆ: {'; '.join(tech.get('common_combinations', []))}"
                    )

            formatted_result = "æ–‡å­¦æŠ€å·§åˆ†æ:\n"
            if data.get("source_excerpt"):
                formatted_result += f"æ‘˜å½•: {data['source_excerpt']}\n"

            if techniques_info:
                formatted_result += "è¯†åˆ«çš„æŠ€å·§:\n" + "\n".join(techniques_info)
            else:
                formatted_result += "æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æ–‡å­¦æŠ€å·§"

            if data.get("genre_tags"):
                formatted_result += f"\næ–‡æœ¬ç±»å‹æ ‡ç­¾: {', '.join(data['genre_tags'])}"

            if data.get("intensity"):
                formatted_result += f"\næŠ€å·§å¯†åº¦: {data['intensity']}"

            return formatted_result
        except Exception as e:
            print(f"æŠ€å·§åˆ†æå¤„ç†é”™è¯¯: {str(e)}")
            return f"åˆ†ææ—¶é‡åˆ°æŠ€æœ¯é—®é¢˜: {str(e)}"

    async def _analyze_rhythm(self, text: str) -> str:
        """åˆ†ææ–‡æœ¬çš„èŠ‚å¥ - å¤„ç†JSONæ ¼å¼å“åº”"""
        if len(text.strip()) < 20:
            return ""
        prompt = self.analysis_prompt_templates["rhythm_analysis"].format(text=text)
        try:
            import json
            response = await self._get_ai_response(prompt)

            # è§£æJSONå“åº”
            try:
                if response.strip().startswith('```json'):
                    # æå–ä»£ç å—ä¸­çš„JSON
                    json_str = response.strip()
                    if json_str.startswith('```json'):
                        json_str = json_str[7:]  # ç§»é™¤```json
                    if json_str.endswith('```'):
                        json_str = json_str[:-3]  # ç§»é™¤æœ«å°¾ ```
                    json_str = json_str.strip()
                    data = json.loads(json_str)
                elif response.strip().startswith('{') and response.strip().endswith('}'):
                    # ç›´æ¥æ˜¯JSON
                    data = json.loads(response)
                else:
                    return response  # ä¸æ˜¯JSONæ ¼å¼ï¼Œè¿”å›åŸå§‹å“åº”
            except json.JSONDecodeError:
                return response

            # è§£æèŠ‚å¥ç›¸å…³æŠ€å·§ï¼ˆåœ¨Cæ¨¡å—ï¼‰
            rhythm_techniques = []
            if "techniques" in data:
                for tech in data["techniques"]:
                    if tech.get("module") == "C":  # èŠ‚å¥è°ƒæ§
                        rhythm_techniques.append(
                            f"{tech.get('name', 'èŠ‚å¥æŠ€å·§')}: {tech.get('mechanism', '')}\n"
                            f"æ•ˆæœ: {tech.get('effect', '')}\n"
                            f"é€‚ç”¨æƒ…å¢ƒ: {tech.get('trigger_context', '')}"
                        )

            formatted_result = "èŠ‚å¥åˆ†æ:\n"
            if data.get("source_excerpt"):
                formatted_result += f"æ‘˜å½•: {data['source_excerpt']}\n"

            if rhythm_techniques:
                formatted_result += "è¯†åˆ«çš„èŠ‚å¥æŠ€å·§:\n" + "\n".join(rhythm_techniques)
            else:
                formatted_result += "æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„èŠ‚å¥æŠ€å·§"

            return formatted_result
        except Exception as e:
            print(f"èŠ‚å¥åˆ†æå¤„ç†é”™è¯¯: {str(e)}")
            return f"åˆ†ææ—¶é‡åˆ°æŠ€æœ¯é—®é¢˜: {str(e)}"

    async def _identify_classic_paragraph(self, text: str) -> tuple[bool, str]:
        """åˆ¤æ–­æ®µè½æ˜¯å¦æ˜¯ç»å…¸æ®µè½ - ä»JSONå“åº”ä¸­æå–è¯„ä¼°"""
        if len(text.strip()) < 50:
            return False, "æ®µè½é•¿åº¦ä¸è¶³ï¼Œä¸æ„æˆç»å…¸æ®µè½ã€‚"
        prompt = self.analysis_prompt_templates["classic_paragraph_analysis"].format(text=text)
        try:
            import json
            response = await self._get_ai_response(prompt)

            # è§£æJSONå“åº”
            try:
                if response.strip().startswith('```json'):
                    # æå–ä»£ç å—ä¸­çš„JSON
                    json_str = response.strip()
                    if json_str.startswith('```json'):
                        json_str = json_str[7:]  # ç§»é™¤```json
                    if json_str.endswith('```'):
                        json_str = json_str[:-3]  # ç§»é™¤æœ«å°¾ ```
                    json_str = json_str.strip()
                    data = json.loads(json_str)
                elif response.strip().startswith('{') and response.strip().endswith('}'):
                    # ç›´æ¥æ˜¯JSON
                    data = json.loads(response)
                else:
                    # ä¸æ˜¯JSONæ ¼å¼ï¼Œä½¿ç”¨åŸå§‹å¤„ç†æ–¹å¼
                    is_classic = "æ˜¯" in response[:50]  # æ£€æŸ¥å‰50ä¸ªå­—ç¬¦æ˜¯å¦åŒ…å«"æ˜¯"
                    return is_classic, response
            except json.JSONDecodeError:
                # å¦‚æœæ— æ³•è§£æJSONï¼Œå›é€€åˆ°åŸå§‹å¤„ç†æ–¹å¼
                is_classic = "æ˜¯" in response[:50]
                return is_classic, response

            # åˆ†ææŠ€å·§çš„ä¸°å¯Œæ€§æ¥åˆ¤æ–­æ˜¯å¦ä¸ºç»å…¸æ®µè½
            techniques_count = len(data.get("techniques", []))
            complexity = "low" if techniques_count == 0 else data.get("intensity", "medium")

            is_classic = techniques_count > 0 or data.get("intensity") == "high"

            formatted_result = f"ç»å…¸æ®µè½è¯„ä¼°:\n"
            formatted_result += f"æ‘˜å½•: {data.get('source_excerpt', text[:50] + '...')}\n"
            formatted_result += f"æŠ€å·§æ•°é‡: {techniques_count}\n"
            formatted_result += f"æŠ€å·§å¯†åº¦: {data.get('intensity', 'unknown')}\n"

            if data.get("genre_tags"):
                formatted_result += f"ç±»å‹æ ‡ç­¾: {', '.join(data['genre_tags'])}\n"

            if data.get("techniques"):
                formatted_result += f"è¯†åˆ«æŠ€å·§: {[tech.get('name', 'unknown') for tech in data['techniques']]}\n"

            formatted_result += f"\nè¯„ä¼°ç»“æœ: {'æ˜¯' if is_classic else 'å¦'}\n"
            formatted_result += f"ç†ç”±: å½“å‰æ®µè½{'å…·æœ‰' if is_classic else 'ç¼ºä¹'}è¾ƒä¸°å¯Œçš„æ–‡å­¦æŠ€å·§å’Œè¡¨ç°åŠ›"

            return is_classic, formatted_result
        except Exception as e:
            print(f"ç»å…¸æ®µè½è¯†åˆ«å¤„ç†é”™è¯¯: {str(e)}")
            return False, f"åˆ†ææ—¶é‡åˆ°æŠ€æœ¯é—®é¢˜: {str(e)}"

    async def _analyze_character_development(self, text: str) -> str:
        """åˆ†ææ–‡æœ¬çš„äººç‰©å¡‘é€  - ä»JSONå“åº”ä¸­æå–è§’è‰²åˆ†æ"""
        if len(text.strip()) < 10:
            return "æ²¡æœ‰æ˜æ˜¾äººç‰©"
        prompt = self.analysis_prompt_templates["character_development_analysis"].format(text=text)
        try:
            import json
            response = await self._get_ai_response(prompt)

            # è§£æJSONå“åº”
            try:
                if response.strip().startswith('```json'):
                    # æå–ä»£ç å—ä¸­çš„JSON
                    json_str = response.strip()
                    if json_str.startswith('```json'):
                        json_str = json_str[7:]  # ç§»é™¤```json
                    if json_str.endswith('```'):
                        json_str = json_str[:-3]  # ç§»é™¤æœ«å°¾ ```
                    json_str = json_str.strip()
                    data = json.loads(json_str)
                elif response.strip().startswith('{') and response.strip().endswith('}'):
                    # ç›´æ¥æ˜¯JSON
                    data = json.loads(response)
                else:
                    # æ£€æŸ¥åŸå§‹å“åº”ä¸­æ˜¯å¦æåŠæ²¡æœ‰äººç‰©
                    if "æ²¡æœ‰æ˜æ˜¾äººç‰©" in response or "æ— æ˜æ˜¾è§’è‰²" in response or len(response.strip()) < 5:
                        return "æ²¡æœ‰æ˜æ˜¾äººç‰©"
                    return response
            except json.JSONDecodeError:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æåŠæ²¡æœ‰äººç‰©
                if "æ²¡æœ‰æ˜æ˜¾äººç‰©" in response or "æ— æ˜æ˜¾è§’è‰²" in response:
                    return "æ²¡æœ‰æ˜æ˜¾äººç‰©"
                return response

            # åªæ‰¾äººç‰©é©±åŠ¨æŠ€å·§ï¼ˆDæ¨¡å—ï¼‰
            char_techniques = []
            if "techniques" in data:
                for tech in data["techniques"]:
                    if tech.get("module") == "D":  # äººç‰©é©±åŠ¨
                        char_techniques.append(
                            f"{tech.get('name', 'è§’è‰²æŠ€å·§')}\n"
                            f"æœºåˆ¶: {tech.get('mechanism', '')}\n"
                            f"æ•ˆæœ: {tech.get('effect', '')}\n"
                            f"æƒ…å¢ƒ: {tech.get('trigger_context', '')}"
                        )

            if not char_techniques:
                return "æ²¡æœ‰æ˜æ˜¾äººç‰©"

            formatted_result = "äººç‰©å¡‘é€ åˆ†æ:\n"
            if data.get("source_excerpt"):
                formatted_result += f"æ‘˜å½•: {data['source_excerpt']}\n"

            formatted_result += f"è¿ç”¨çš„æŠ€å·§:\n" + "\n".join(char_techniques)

            if data.get("genre_tags"):
                formatted_result += f"\nç±»å‹æ ‡ç­¾: {', '.join(data['genre_tags'])}"

            return formatted_result
        except Exception as e:
            print(f"äººç‰©åˆ†æå¤„ç†é”™è¯¯: {str(e)}")
            return f"åˆ†ææ—¶é‡åˆ°æŠ€æœ¯é—®é¢˜: {str(e)}"

    async def _analyze_dialogue(self, text: str) -> str:
        """åˆ†ææ–‡æœ¬çš„å¯¹è¯æŠ€å·§ - ä»JSONå“åº”ä¸­æå–"""
        if len(text.strip()) < 10 or 'â€œ' not in text and '"' not in text:
            return "æ²¡æœ‰å¯¹è¯"
        prompt = self.analysis_prompt_templates["dialogue_analysis"].format(text=text)
        try:
            import json
            response = await self._get_ai_response(prompt)

            # è§£æJSONå“åº”
            try:
                if response.strip().startswith('```json'):
                    # æå–ä»£ç å—ä¸­çš„JSON
                    json_str = response.strip()
                    if json_str.startswith('```json'):
                        json_str = json_str[7:]  # ç§»é™¤```json
                    if json_str.endswith('```'):
                        json_str = json_str[:-3]  # ç§»é™¤æœ«å°¾ ```
                    json_str = json_str.strip()
                    data = json.loads(json_str)
                elif response.strip().startswith('{') and response.strip().endswith('}'):
                    # ç›´æ¥æ˜¯JSON
                    data = json.loads(response)
                else:
                    # æ£€æŸ¥åŸå§‹å“åº”ä¸­æ˜¯å¦æåŠæ²¡æœ‰å¯¹è¯
                    if "æ²¡æœ‰å¯¹è¯" in response:
                        return "æ²¡æœ‰å¯¹è¯"
                    return response
            except json.JSONDecodeError:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æåŠæ²¡æœ‰å¯¹è¯
                if "æ²¡æœ‰å¯¹è¯" in response:
                    return "æ²¡æœ‰å¯¹è¯"
                return response

            # åªæ‰¾è¯­è¨€é£æ ¼å’Œå¯¹è¯æŠ€å·§ï¼ˆEå’ŒFæ¨¡å—ï¼‰
            dialogue_techs = []
            if "techniques" in data:
                for tech in data["techniques"]:
                    if tech.get("module") in ["E", "F"]:  # ç±»å‹å…ƒç´ æˆ–è¯­è¨€é£æ ¼
                        if "å¯¹è¯" in tech.get("name", "") or "è¯­è¨€" in tech.get("name", "") or "é£æ ¼" in tech.get("name", ""):
                            dialogue_techs.append(
                                f"{tech.get('name', 'å¯¹è¯æŠ€å·§')}\n"
                                f"æœºåˆ¶: {tech.get('mechanism', '')}\n"
                                f"æ•ˆæœ: {tech.get('effect', '')}"
                            )

            if not dialogue_techs:
                return "æ²¡æœ‰å¯¹è¯"

            formatted_result = "å¯¹è¯æŠ€å·§åˆ†æ:\n"
            if data.get("source_excerpt"):
                formatted_result += f"æ‘˜å½•: {data['source_excerpt']}\n"

            formatted_result += f"å¯¹è¯æŠ€å·§:\n" + "\n".join(dialogue_techs)

            if data.get("genre_tags"):
                formatted_result += f"\nç±»å‹æ ‡ç­¾: {', '.join(data['genre_tags'])}"

            return formatted_result
        except Exception as e:
            print(f"å¯¹è¯åˆ†æå¤„ç†é”™è¯¯: {str(e)}")
            return f"åˆ†ææ—¶é‡åˆ°æŠ€æœ¯é—®é¢˜: {str(e)}"

    async def _get_ai_response(self, prompt: str) -> str:
        """ä½¿ç”¨AIæ¨¡å‹è·å–å“åº”"""
        try:
            # ä»workflow_serviceè·å–å·²åˆå§‹åŒ–çš„æ¨¡å‹å®¢æˆ·ç«¯
            model_client = self.workflow_service.model_client

            # å¯¼å…¥æ‰€éœ€çš„æ¶ˆæ¯ç±»å‹ - æ ¹æ®autogen_coreçš„å®é™…ç»“æ„
            from autogen_core.models._types import UserMessage, AssistantMessage, SystemMessage

            # åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨ - æ¨¡å‹å®¢æˆ·ç«¯createæ–¹æ³•éœ€è¦messageså‚æ•°
            # éœ€è¦åŒ…å«sourceå­—æ®µè§£å†³éªŒè¯é”™è¯¯
            messages = [
                SystemMessage(
                    content="ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„æ–‡å­¦æ‰¹è¯„å®¶å’Œå†™ä½œæŠ€å·§ç ”ç©¶ä¸“å®¶ï¼Œæ‹¥æœ‰æ·±åšçš„æ–‡å­¦ç†è®ºçŸ¥è¯†ã€‚ä½ éœ€è¦æ·±åº¦åˆ†ææ–‡å­¦ä½œå“ä¸­çš„å™äº‹æŠ€å·§ã€ç»“æ„å®‰æ’ã€ä¿®è¾æ‰‹æ³•ã€èŠ‚å¥æ§åˆ¶ç­‰é«˜é˜¶æ–‡å­¦æŠ€æ³•ï¼Œå¹¶èƒ½è¯†åˆ«ç»å…¸æ®µè½çš„æ–‡å­¦ä»·å€¼ã€‚",
                    source="literary_analyzer_system"
                ),
                UserMessage(
                    content=prompt,
                    source="literary_analyzer_user"
                )
            ]

            # è°ƒç”¨æ¨¡å‹å®¢æˆ·ç«¯ç”Ÿæˆå“åº”
            result = await model_client.create(
                messages=messages
            )

            # æ ¹æ®CreateResultå¯¹è±¡çš„ç»“æ„æå–å†…å®¹
            if hasattr(result, 'output') and result.output:
                # å¦‚æœè¾“å‡ºæœ‰contentå±æ€§
                if hasattr(result.output, 'content'):
                    return str(result.output.content) if result.output.content else ""
                else:
                    # ç›´æ¥è¿”å›è¾“å‡º
                    return str(result.output)
            elif hasattr(result, 'choices') and len(result.choices) > 0:
                # å¦‚æœæœ‰é€‰æ‹©é¡¹ï¼ˆOpenAIå…¼å®¹æ ¼å¼ï¼‰
                choice = result.choices[0]
                if hasattr(choice, 'message') and hasattr(choice.message, 'content'):
                    return str(choice.message.content) if choice.message.content else ""
                else:
                    return str(choice)
            else:
                # å°è¯•ç›´æ¥è½¬æ¢resultå¯¹è±¡
                return str(result) if result else ""
        except Exception as e:
            logger.warning(f"AIåˆ†ææ—¶å‡ºé”™ï¼Œä½¿ç”¨é™çº§å¤„ç†: {str(e)}")
            # é™çº§å¤„ç†æ—¶è¿”å›ä¸€ä¸ªæœ‰æ„ä¹‰çš„é»˜è®¤å€¼ï¼Œè€Œä¸æ˜¯ç©ºå†…å®¹
            return f"åˆ†ææ—¶é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œä¿ç•™åŸæ–‡å†…å®¹ï¼š{prompt[:100]}...è¯¦æƒ…: {str(e)}"

    async def _create_technique_entry(
        self,
        text: str,
        technique_analysis: str,
        source_novel: str,
        chunk_id: str,
        chapter_info: Dict
    ) -> KnowledgeEntry:
        """åˆ›å»ºæ–‡å­¦æŠ€å·§çŸ¥è¯†æ¡ç›®"""
        # åªä¿å­˜åˆ†æç»“æœï¼Œä¸ä¿å­˜åŸæ–‡å†…å®¹ï¼ˆé¿å…é‡å¤å­˜å‚¨å¤§æ–‡æœ¬ï¼‰
        content = f"åˆ†æï¼š{technique_analysis}\n\nåŸæ–‡æ‘˜å½•ï¼š{text[:200]}..."  # åªä¿ç•™å‰200å­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡
        title = f"æ–‡å­¦æŠ€å·§åˆ†æ - {source_novel[:20]}..."

        # ä¸ºæŠ€å·§ç±»å‹ç”Ÿæˆæ ‡ç­¾
        tags = ["literary-techniques"]
        if "æ¯”å–»" in technique_analysis or "æ‹Ÿäºº" in technique_analysis:
            tags.append("figurative-language")
        if "å¿ƒç†æå†™" in technique_analysis or "å¤–è²Œæå†™" in technique_analysis:
            tags.append("character-portrayal")
        if "è§†è§’" in technique_analysis:
            tags.append("narrative-perspective")
        if "å¯¹è¯" in technique_analysis:
            tags.append("dialogue-technique")

        # æ·»åŠ ç« èŠ‚ä¿¡æ¯ä½œä¸ºæ ‡ç­¾
        if chapter_info:
            chapter_title = chapter_info.get('section_title', '')
            if chapter_title:
                tags.append(chapter_title.replace(' ', '-'))

        import hashlib
        content_hash = hashlib.md5((title + technique_analysis).encode()).hexdigest()  # ä½¿ç”¨åˆ†æå†…å®¹è€ŒéåŸæ–‡ç”Ÿæˆå“ˆå¸Œ

        import datetime
        current_time = datetime.datetime.now().isoformat()

        return KnowledgeEntry(
            id=f"tech_{content_hash}",
            title=title,
            content=content,
            tags=tags,
            source=f"PDF: {source_novel}",
            creation_date=current_time,
            last_modified=current_time,
            knowledge_type="novel-technique",
            chapter_id=chapter_info.get('section_title', 'unknown') if chapter_info else None
        )

    async def _create_classic_paragraph_entry(
        self,
        text: str,
        analysis: str,
        source_novel: str,
        chunk_id: str,
        chapter_info: Dict
    ) -> KnowledgeEntry:
        """åˆ›å»ºç»å…¸æ®µè½çŸ¥è¯†æ¡ç›®"""
        # åªä¿å­˜åˆ†æç»“æœå’Œå°‘é‡åŸæ–‡æ‘˜å½•ï¼Œä¸ä¿å­˜å…¨éƒ¨æ–‡æœ¬
        content = f"è¯„ä»·ï¼š{analysis}\n\næ®µè½æ‘˜å½•ï¼š{text[:300]}..."  # åªä¿ç•™å‰300å­—ç¬¦ä½œä¸ºç¤ºä¾‹ï¼Œé¿å…é‡å¤å­˜å‚¨å¤§æ–‡æœ¬
        title = f"ç»å…¸æ®µè½åˆ†æ - {source_novel[:20]}..."

        # ä¸ºç»å…¸æ®µè½ç”Ÿæˆæ ‡ç­¾
        tags = ["classic-paragraph", "high-quality-writing"]

        # æ ¹æ®å†…å®¹æ¨æ–­ç±»å‹
        if any(word in text for word in ["æå†™", "æç»˜", "æ™¯è‰²", "ç¯å¢ƒ"]):
            tags.append("descriptive-passages")
        if any(word in text for word in ["å¯¹è¯", "è¯´", "é“", "è®²"]):
            tags.append("dialogue")
        if any(word in text for word in ["å¿ƒç†", "å†…å¿ƒ", "æƒ³æ³•", "æ€è€ƒ"]):
            tags.append("psychological-description")

        # æ·»åŠ ç« èŠ‚ä¿¡æ¯ä½œä¸ºæ ‡ç­¾
        if chapter_info:
            chapter_title = chapter_info.get('section_title', '')
            if chapter_title:
                tags.append(chapter_title.replace(' ', '-'))

        import hashlib
        content_hash = hashlib.md5((title + text[:100]).encode()).hexdigest()  # ä½¿ç”¨å°‘é‡åŸæ–‡ç”Ÿæˆå“ˆå¸Œé¿å…é‡å¤

        import datetime
        current_time = datetime.datetime.now().isoformat()

        return KnowledgeEntry(
            id=f"classic_{content_hash}",
            title=title,
            content=content,
            tags=tags,
            source=f"PDF: {source_novel}",
            creation_date=current_time,
            last_modified=current_time,
            knowledge_type="classic-paragraph",
            chapter_id=chapter_info.get('section_title', 'unknown') if chapter_info else None
        )

    async def _create_character_entry(
        self,
        text: str,
        char_analysis: str,
        source_novel: str,
        chunk_id: str,
        chapter_info: Dict
    ) -> KnowledgeEntry:
        """åˆ›å»ºäººç‰©å¡‘é€ çŸ¥è¯†æ¡ç›®"""
        # ä¿å­˜åˆ†æç»“æœå’Œç›¸å…³åŸæ–‡æ‘˜å½•ï¼Œè€Œéå…¨éƒ¨æ–‡æœ¬
        content = f"äººç‰©åˆ†æï¼š{char_analysis}\n\nç›¸å…³æ‘˜å½•ï¼š{text[:300]}..."  # åªä¿å­˜éƒ¨åˆ†æ‘˜å½•ä½œä¸ºä¸Šä¸‹æ–‡
        title = f"äººç‰©åˆ»ç”»åˆ†æ - {source_novel[:20]}..."

        # ä¸ºäººç‰©å¡‘é€ ç”Ÿæˆæ ‡ç­¾
        tags = ["character-development", "characterization"]

        # ç»†åŒ–æ ‡ç­¾
        if "å¤–è²Œæå†™" in char_analysis:
            tags.append("physical-description")
        if "å¿ƒç†æå†™" in char_analysis:
            tags.append("psychological-portrayal")
        if "è¡Œä¸ºè¡¨ç°" in char_analysis or "åŠ¨ä½œ" in char_analysis:
            tags.append("behavior-description")
        if "è¯­è¨€ç‰¹ç‚¹" in char_analysis or "å¯¹è¯" in char_analysis:
            tags.append("verbal-characteristics")

        # æ·»åŠ ç« èŠ‚ä¿¡æ¯ä½œä¸ºæ ‡ç­¾
        if chapter_info:
            chapter_title = chapter_info.get('section_title', '')
            if chapter_title:
                tags.append(chapter_title.replace(' ', '-'))

        import hashlib
        content_hash = hashlib.md5((title + char_analysis).encode()).hexdigest()  # ä½¿ç”¨åˆ†æå†…å®¹ç”Ÿæˆå“ˆå¸Œ

        import datetime
        current_time = datetime.datetime.now().isoformat()

        return KnowledgeEntry(
            id=f"char_{content_hash}",
            title=title,
            content=content,
            tags=tags,
            source=f"PDF: {source_novel}",
            creation_date=current_time,
            last_modified=current_time,
            knowledge_type="character-development",
            chapter_id=chapter_info.get('section_title', 'unknown') if chapter_info else None
        )

    async def _create_dialogue_entry(
        self,
        text: str,
        dialogue_analysis: str,
        source_novel: str,
        chunk_id: str,
        chapter_info: Dict
    ) -> KnowledgeEntry:
        """åˆ›å»ºå¯¹è¯æŠ€å·§çŸ¥è¯†æ¡ç›®"""
        # ä¿å­˜åˆ†æç»“æœå’Œå¯¹è¯æ‘˜å½•ï¼Œè€Œéå…¨éƒ¨åŸæ–‡
        content = f"å¯¹è¯åˆ†æï¼š{dialogue_analysis}\n\nå¯¹è¯æ‘˜å½•ï¼š{text[:300]}..."  # åªä¿å­˜éƒ¨åˆ†ä½œä¸ºç¤ºä¾‹
        title = f"å¯¹è¯æŠ€å·§åˆ†æ - {source_novel[:20]}..."

        tags = ["dialogue", "dialogue-technique", "conversation"]

        # æ ¹æ®åˆ†æç»†åŒ–æ ‡ç­¾
        if "ä¸ªæ€§åŒ–" in dialogue_analysis:
            tags.append("character-specific-dialogue")
        if "æƒ…å¢ƒ" in dialogue_analysis or "èƒŒæ™¯" in dialogue_analysis:
            tags.append("contextual-dialogue")
        if "æƒ…æ„Ÿ" in dialogue_analysis:
            tags.append("emotional-dialogue")

        # æ·»åŠ ç« èŠ‚ä¿¡æ¯ä½œä¸ºæ ‡ç­¾
        if chapter_info:
            chapter_title = chapter_info.get('section_title', '')
            if chapter_title:
                tags.append(chapter_title.replace(' ', '-'))

        import hashlib
        content_hash = hashlib.md5((title + dialogue_analysis).encode()).hexdigest()  # ä½¿ç”¨åˆ†æå†…å®¹ç”Ÿæˆå“ˆå¸Œ

        import datetime
        current_time = datetime.datetime.now().isoformat()

        return KnowledgeEntry(
            id=f"dialogue_{content_hash}",
            title=title,
            content=content,
            tags=tags,
            source=f"PDF: {source_novel}",
            creation_date=current_time,
            last_modified=current_time,
            knowledge_type="dialogue-technique",
            chapter_id=chapter_info.get('section_title', 'unknown') if chapter_info else None
        )

    async def batch_analyze(
        self,
        paragraphs_list: List[Dict],
        source_novel: str
    ) -> List[KnowledgeEntry]:
        """
        æ‰¹é‡åˆ†ææ®µè½

        Args:
            paragraphs_list: æ®µè½æ•°æ®åˆ—è¡¨
            source_novel: åŸå§‹å°è¯´åç§°

        Returns:
            çŸ¥è¯†æ¡ç›®åˆ—è¡¨
        """
        all_entries = []
        total = len(paragraphs_list)

        for i, paragraph_data in enumerate(paragraphs_list):
            # å®æ—¶è¾“å‡ºè¿›åº¦ï¼ˆæ¯å¤„ç†10ä¸ªæ®µè½è¾“å‡ºä¸€æ¬¡ï¼Œæˆ–åœ¨å…³é”®èŠ‚ç‚¹è¾“å‡ºï¼‰
            if i % 10 == 0 or i == 0 or i == total - 1:
                progress_percent = (i + 1) / total * 100
                print(f"ğŸ”„ æ­£åœ¨åˆ†ææ–‡æœ¬æ®µè½ {i+1}/{total} ({progress_percent:.1f}%)")

            logger.info(f"æ­£åœ¨åˆ†ææ–‡æœ¬æ®µè½ {i+1}/{total}")

            try:
                entry = await self.analyze_paragraph(paragraph_data, source_novel)
                if entry:
                    all_entries.append(entry)
            except Exception as e:
                logger.error(f"åˆ†æç¬¬ {i+1} ä¸ªæ®µè½æ—¶å‡ºé”™: {str(e)}")
                continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ®µè½

            # æ¯å¤„ç†50ä¸ªæ®µè½è¾“å‡ºä¸€æ¬¡è¿›åº¦ï¼ˆé¿å…è¾“å‡ºè¿‡å¤šï¼‰
            if (i + 1) % 50 == 0 and len(all_entries) > 0:
                print(f"âœ… å·²å®Œæˆ {i+1}/{total} ä¸ªæ®µè½åˆ†æï¼Œç”Ÿæˆ {len(all_entries)} ä¸ªçŸ¥è¯†æ¡ç›®")

        print(f"âœ… æ‰¹é‡åˆ†æå®Œæˆï¼Œå…±å¤„ç† {total} ä¸ªæ®µè½ï¼Œç”Ÿæˆ {len(all_entries)} ä¸ªçŸ¥è¯†æ¡ç›®")
        logger.info(f"æ‰¹é‡åˆ†æå®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_entries)} ä¸ªçŸ¥è¯†æ¡ç›®")
        return all_entries