"""
PDFå°è¯´å¤„ç†æ¨¡å—

åŠŸèƒ½:
- PDFæ–‡ä»¶è¯»å–å’Œé¢„å¤„ç†
- å†…å®¹åˆ†å—æå–ï¼ˆæŒ‰æ®µè½ã€ç« èŠ‚ï¼‰
- æå–å°è¯´å…ƒä¿¡æ¯
- æä¾›æ ‡å‡†åŒ–çš„æ–‡æ¡£ç»“æ„ç”¨äºåç»­åˆ†æ
"""
import asyncio
import logging
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import fitz  # PyMuPDF - æ›´å¥½çš„PDFå¤„ç†åº“
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PDFProcessor:
    """PDFå¤„ç†å™¨ - ä¸“é—¨ç”¨äºå¤„ç†å°è¯´PDFæ–‡ä»¶"""

    def __init__(self):
        # å¸¸è§çš„å°è¯´ç« èŠ‚æ ‡è¯†
        self.chapter_patterns = [
            r'^ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+)\s*(å›|ç« |èŠ‚)',  # ç¬¬Nç« /å›/èŠ‚
            r'^CHAPTER\s+([IVXLCDM\d]+)',  # CHAPTER N
            r'^Chapter\s+([IVXLCDM\d]+)',  # Chapter N
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+\s*[ã€.]\s*',  # æ±‰å­—åºå·+ã€æˆ–.
            r'^\d+\s*[ã€.]\s*[^\d]',      # é˜¿æ‹‰ä¼¯æ•°å­—+ã€æˆ–. å¼€å¤´æ–‡æœ¬
            r'Part\s+[IVXLCDM\d]+',       # Part N
            r'^[A-Z][a-z]+\s+\d+',        # è‹±æ–‡å•è¯+æ•°å­—
        ]

    def extract_pdf_content(self, pdf_path: str) -> Dict:
        """
        æå–PDFå†…å®¹å¹¶è¿”å›ç»“æ„åŒ–æ•°æ®

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            åŒ…å«PDFåŸºæœ¬ä¿¡æ¯å’Œå†…å®¹çš„å­—å…¸
        """
        doc = None
        try:
            doc = fitz.open(pdf_path)

            # æå–å…ƒä¿¡æ¯
            try:
                metadata = doc.metadata
                title = metadata.get('title', '') or Path(pdf_path).stem
                author = metadata.get('author', '')
            except:
                # å¦‚æœæ— æ³•è·å–å…ƒä¿¡æ¯ï¼Œä½¿ç”¨æ–‡ä»¶å
                title = Path(pdf_path).stem
                author = 'Unknown'

            # æå–æ–‡æœ¬å†…å®¹
            content_parts = []
            current_page_num = 0

            for page_num in range(doc.page_count):
                current_page_num = page_num
                try:
                    page = doc.load_page(page_num)
                    text = page.get_text("text")

                    # ç¡®ä¿æ–‡æ¡£ä»ç„¶æ‰“å¼€
                    if doc.is_closed:
                        logger.error(f"PDFæ–‡æ¡£æ„å¤–å…³é—­: {pdf_path}")
                        break

                    if text.strip():  # å¿½ç•¥ç©ºé¡µ
                        content_parts.append(text)
                except Exception as e:
                    logger.warning(f"è¯»å–é¡µé¢ {page_num} æ—¶å‡ºé”™: {str(e)}, è·³è¿‡è¯¥é¡µé¢")
                    continue

            content = "\n".join(content_parts)

            # ç”Ÿæˆä¿¡æ¯
            result = {
                'title': title,
                'author': author,
                'content': content,
                'total_pages': doc.page_count,
                'file_path': pdf_path
            }

            doc.close()
            return result

        except Exception as e:
            logger.error(f"å¤„ç†PDFæ–‡ä»¶æ—¶å‡ºé”™: {pdf_path}, é”™è¯¯: {str(e)}")
            if doc and not doc.is_closed:
                try:
                    doc.close()
                except:
                    pass
            raise

    def segment_content(self, content: str) -> List[Dict]:
        """
        å°†PDFå†…å®¹æŒ‰æ®µè½å’Œç« èŠ‚åˆ†æ®µ

        Args:
            content: PDFçš„åŸå§‹æ–‡æœ¬å†…å®¹

        Returns:
            åˆ†æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«æ®µè½å†…å®¹ã€ä½ç½®ç­‰ä¿¡æ¯
        """
        # æ¸…ç†æ–‡æœ¬ - ç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦å’Œç©ºæ ¼
        print("ğŸ§¹ æ­£åœ¨æ¸…ç†æ–‡æœ¬å†…å®¹...")
        cleaned_content = self._clean_content(content)

        # æŒ‰ç…§æ®µè½è¿›è¡Œåˆ†å‰²
        print("âœ‚ï¸  æ­£åœ¨æŒ‰æ®µè½åˆ†å‰²å†…å®¹...")
        paragraphs = self._split_into_paragraphs(cleaned_content)
        total_paragraphs = len(paragraphs)
        print(f"âœ… æ®µè½åˆ†å‰²å®Œæˆï¼Œå…± {total_paragraphs} ä¸ªæ®µè½")

        # è¯†åˆ«ç« èŠ‚åˆ’åˆ†
        print("ì±• æ­£åœ¨è¯†åˆ«ç« èŠ‚åˆ’åˆ†...")
        segmented_paragraphs = self._identify_chapters(paragraphs)

        # ä¸ºæ¯ä¸ªæ®µè½åˆ›å»ºç»“æ„åŒ–ä¿¡æ¯
        result = []
        for i, (text, chapter_info) in enumerate(segmented_paragraphs):
            paragraph_info = {
                'id': f'pdf_para_{i:04d}',
                'text': text,
                'original_pos': i,
                'chapter_info': chapter_info,
                'word_count': len(text.strip()),
                'is_chapter_header': chapter_info.get('is_header', False),
                'section_title': chapter_info.get('section_title', '')
            }
            result.append(paragraph_info)

            # æ¯å¤„ç†100ä¸ªæ®µè½æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (i + 1) % 100 == 0:
                progress = (i + 1) / total_paragraphs * 100
                print(f"ğŸ“ å·²å¤„ç† {i+1}/{total_paragraphs} ä¸ªæ®µè½ ({progress:.1f}%)")

        logger.info(f"å†…å®¹å·²åˆ†å‰²ä¸º {len(result)} ä¸ªæ®µè½æ®µ")
        print(f"âœ… å†…å®¹å·²åˆ†å‰²ä¸º {len(result)} ä¸ªåˆ†æ®µæ¡ç›®")
        return result

    def _clean_content(self, content: str) -> str:
        """æ¸…ç†PDFå†…å®¹æ–‡æœ¬"""
        # æ›¿æ¢è¿ç»­çš„ç©ºç™½å­—ç¬¦ä¸ºå•ä¸ªç©ºæ ¼ï¼Œä¿ç•™é‡è¦çš„æ®µè½åˆ†å‰²ç¬¦
        content = re.sub(r'\r', '\n', content)  # ç»Ÿä¸€æ¢è¡Œç¬¦

        # ä¿ç•™æ®µè½çº§çš„åˆ†å‰²ï¼ˆä¸¤ä¸ªæˆ–å¤šä¸ªæ¢è¡Œï¼‰
        content = re.sub(r'\n{3,}', '\n\n', content)

        # å‹ç¼©å•ä¸ªæ¢è¡Œå’Œè¿å­—ç¬¦ï¼ˆå¦‚æ¢è¡Œå¤„æ–­å¼€çš„å•è¯ï¼‰
        lines = content.split('\n')
        processed_lines = []

        for i, line in enumerate(lines):
            line = line.strip()
            if line:
                # æ£€æŸ¥å½“å‰è¡Œæ˜¯å¦å¾ˆçŸ­ï¼ˆå¯èƒ½å¯¼è‡´è¿è¯åˆ†å‰²é”™è¯¯ï¼‰
                if len(line) > 10:
                    processed_lines.append(line)
                # å¦‚æœè¡Œæœ«ä»¥è¿å­—ç¬¦ç»“æŸï¼Œå¹¶ä¸”ä¸æ˜¯ç« èŠ‚æ ‡é¢˜ï¼Œåˆ™å¯èƒ½æ˜¯ä¸€ä¸ªè¢«æ‹†å¼€çš„è¯
                elif line.endswith('-') and len(line) > 1 and not self._is_chapter_title(line):
                    if i < len(lines) - 1:
                        # ä¸ä¸‹ä¸€è¡Œåˆå¹¶
                        if processed_lines:  # å¦‚æœä¹‹å‰æœ‰è¡Œè¢«å¤„ç†
                            processed_lines[-1] = processed_lines[-1][:-1] + lines[i+1].lstrip()
                        continue  # è·³è¿‡å½“å‰è¡Œ
                else:
                    processed_lines.append(line)

        # é‡æ–°ç»„åˆæ®µè½
        final_content = '\n'.join(processed_lines)

        # å†æ¬¡åˆå¹¶è¿‡å¤šçš„ç©ºè¡Œ
        final_content = re.sub(r'\n{3,}', '\n\n', final_content)

        return final_content

    def _split_into_paragraphs(self, content: str) -> List[str]:
        """æŒ‰æ®µè½åˆ†å‰²å†…å®¹ï¼Œå¯¹äºé•¿æ®µè½è¿›è¡Œåˆ†å‰²"""
        # æŒ‰åŒæ¢è¡Œåˆ†å‰²æ®µè½
        paragraphs = content.split('\n\n')

        # è¿‡æ»¤æ‰ç©ºçš„æˆ–å¤ªçŸ­çš„å†…å®¹
        paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p.strip()) > 10]

        # æ™ºèƒ½åˆ†å‰²è¿‡é•¿æ®µè½
        final_paragraphs = []
        for para in paragraphs:
            if len(para) > 500 and not self._is_chapter_title(para):  # å¤ªé•¿çš„æ®µè½è¿›è¡Œåˆ†å‰²
                # æŒ‰å¥å­ç»“æŸç¬¦è¿›è¡Œå­åˆ†å‰²
                sentences = re.split(r'([ã€‚ï¼ï¼Ÿ!?]+)', para)
                sub_paragraphs = []

                current_para = ""
                for sentence in sentences:
                    if len(current_para + sentence) < 400:
                        current_para += sentence
                    else:
                        if current_para.strip():
                            sub_paragraphs.append(current_para.strip())
                        current_para = sentence

                if current_para.strip():
                    sub_paragraphs.append(current_para.strip())

                # åˆ†å‰²å¤ªé•¿çš„éƒ¨åˆ†ï¼Œé¿å…AIå¤„ç†è¶…é™
                for sub_para in sub_paragraphs:
                    if len(sub_para) > 500:
                        # æŒ‰å›ºå®šé•¿åº¦åˆ‡åˆ†
                        for i in range(0, len(sub_para), 400):
                            chunk = sub_para[i:i+400]
                            final_paragraphs.append(chunk)
                    else:
                        final_paragraphs.append(sub_para)
            else:
                if len(para) < 50 and final_paragraphs:  # è¿‡çŸ­çš„å†…å®¹åˆå¹¶
                    # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯ç« èŠ‚æ ‡é¢˜
                    prev_para = final_paragraphs[-1]
                    if self._is_chapter_title(para) or len(para) < 20:  # ç« èŠ‚æ ‡é¢˜å•ç‹¬ä¿ç•™
                        final_paragraphs.append(para)
                    else:  # åˆå¹¶åˆ°å‰ä¸€æ®µè½
                        final_paragraphs[-1] = prev_para + " " + para
                else:
                    final_paragraphs.append(para)

        return final_paragraphs

    def _identify_chapters(self, paragraphs: List[str]) -> List[Tuple[str, Dict]]:
        """è¯†åˆ«æ–‡æœ¬ä¸­çš„ç« èŠ‚åˆ’åˆ†"""
        result = []

        for para in paragraphs:
            chapter_info = {'is_header': False, 'section_title': ''}

            if self._is_chapter_title(para):
                chapter_info['is_header'] = True
                chapter_info['section_title'] = para.strip()

            result.append((para, chapter_info))

        # æ›´æ–°æ‰€æœ‰æ®µè½çš„ç« èŠ‚å½’å±ä¿¡æ¯
        final_result = []
        current_chapter = "å¼€å¤´éƒ¨åˆ†"

        for para, info in result:
            if info['is_header']:
                current_chapter = info['section_title']
                final_result.append((para, info))
            else:
                info['current_chapter'] = current_chapter
                final_result.append((para, info))

        return final_result

    def _is_chapter_title(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜"""
        text = text.strip()
        for pattern in self.chapter_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                return True
        return False

    def process_pdf(self, pdf_path: str) -> List[Dict]:
        """
        å®Œæ•´çš„PDFå¤„ç†æµç¨‹

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            ç»è¿‡åˆ†æ®µçš„PDFå†…å®¹åˆ—è¡¨
        """
        print(f"ğŸ“– å¼€å§‹å¤„ç†PDFæ–‡ä»¶: {Path(pdf_path).name}")
        logger.info(f"å¼€å§‹å¤„ç†PDF: {pdf_path}")

        # æå–å®Œæ•´å†…å®¹
        print("ğŸ” æ­£åœ¨æå–PDFå†…å®¹...")
        pdf_data = self.extract_pdf_content(pdf_path)
        title = pdf_data['title']
        content = pdf_data['content']
        total_pages = pdf_data['total_pages']

        print(f"âœ… PDFå†…å®¹æå–å®Œæˆ ({total_pages} é¡µ)")

        # åˆ†æ®µå¤„ç†
        print("âœ‚ï¸  æ­£åœ¨å¯¹å†…å®¹è¿›è¡Œåˆ†æ®µ...")
        segmented_content = self.segment_content(content)

        # æ·»åŠ å°è¯´æ ‡é¢˜åˆ°æ¯æ®µ
        for segment in segmented_content:
            segment['original_title'] = title

        print(f"âœ… PDFå†…å®¹åˆ†æ®µå®Œæˆï¼Œå…±åˆ†å‰²å‡º {len(segmented_content)} ä¸ªæ®µè½")
        logger.info(f"PDFå¤„ç†å®Œæˆï¼Œå…±åˆ†å‰²å‡º {len(segmented_content)} ä¸ªæ®µè½æ®µ")
        return segmented_content


# ä½¿ç”¨ç¤ºä¾‹ï¼ˆå¯é€‰æµ‹è¯•æ–¹æ³•ï¼‰
if __name__ == "__main__":
    processor = PDFProcessor()
    # ç¤ºä¾‹ç”¨æ³•ï¼š
    # processed_data = processor.process_pdf("example_novel.pdf")
    # print(f"å·²æå–: {len(processed_data)} ä¸ªæ®µè½")