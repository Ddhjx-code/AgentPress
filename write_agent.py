# main.py
import asyncio
import os
import json
import re
from pathlib import Path
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core.models import ModelInfo, ModelFamily

def load_prompt(file_path: str) -> str:
    """ä» Markdown æ–‡ä»¶åŠ è½½æç¤ºè¯"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
        # ç§»é™¤ Markdown æ ‡é¢˜å’Œä»£ç å—æ ‡è®°
        content = re.sub(r'^#+\s*', '', content, flags=re.MULTILINE)
        content = re.sub(r'```[a-z]*\n?', '', content)
        return content.strip()

def extract_content(messages):
    """é€šç”¨å†…å®¹æå–"""
    for msg in reversed(messages):
        if hasattr(msg, 'content') and isinstance(msg.content, str):
            return msg.content
    return ""

def parse_json_response(response: str) -> dict:
    """å®‰å…¨è§£æ JSON å“åº”"""
    try:
        # æå– JSON éƒ¨åˆ†ï¼ˆå…¼å®¹ markdown ä»£ç å—ï¼‰
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
        else:
            return json.loads(response)
    except (json.JSONDecodeError, TypeError):
        print(f"âš ï¸ JSON è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹")
        return {"raw_response": response}

async def main():
    print("ğŸš€ å¯åŠ¨ç»“æ„åŒ–å±±æµ·ç»ç¼–è¾‘ç¤¾...")
    
    # åŠ è½½æç¤ºè¯
    prompt_dir = Path("prompts")
    prompts = {
        "mythologist": load_prompt(prompt_dir / "mythologist.md"),
        "writer": load_prompt(prompt_dir / "writer.md"),
        "fact_checker": load_prompt(prompt_dir / "fact_checker.md"),
        "editor": load_prompt(prompt_dir / "editor.md")
    }

    # æ¨¡å‹é…ç½®
    model_client = OpenAIChatCompletionClient(
        model="qwen-max",
        api_key=os.getenv("QWEN_API_KEY"),
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        model_info=ModelInfo(
            vision=False,
            function_calling=True,
            json_output=True,
            structured_output=False,
            family=ModelFamily.GPT_5
        )
    )

    # åˆ›å»ºè§’è‰²
    agents = {
        "mythologist": AssistantAgent("Mythologist", model_client=model_client, system_message=prompts["mythologist"]),
        "writer": AssistantAgent("Writer", model_client=model_client, system_message=prompts["writer"]),
        "fact_checker": AssistantAgent("FactChecker", model_client=model_client, system_message=prompts["fact_checker"]),
        "editor": AssistantAgent("Editor", model_client=model_client, system_message=prompts["editor"])
    }

    # åŸæ–‡è¾“å…¥
    shanhai_text = "é’ä¸˜ä¹‹å±±ï¼Œæœ‰å…½ç„‰ï¼Œå…¶çŠ¶å¦‚ç‹è€Œä¹å°¾ï¼Œå…¶éŸ³å¦‚å©´å„¿ï¼Œèƒ½é£Ÿäººï¼Œé£Ÿè€…ä¸è›Šã€‚"
    print(f"\nğŸ“ åŸæ–‡: {shanhai_text}")

    # Step 1: ç¥è¯å­¦å®¶è€ƒæ®
    print("\nğŸ” Step 1: ç¥è¯å­¦å®¶è€ƒæ®...")
    myth_task = f"è¯·åˆ†æä»¥ä¸‹ã€Šå±±æµ·ç»ã€‹åŸæ–‡ï¼š{shanhai_text}"
    myth_result = await agents["mythologist"].run(task=myth_task)
    myth_content = extract_content(myth_result.messages)
    research_data = parse_json_response(myth_content)
    print(f"âœ… è€ƒæ®å®Œæˆ: {research_data.get('translation', 'N/A')[:100]}...")

    # Step 2: ä½œå®¶åˆ›ä½œåˆç¨¿
    print("\nâœï¸ Step 2: ä½œå®¶åˆ›ä½œåˆç¨¿...")
    writer_input = json.dumps({"research": research_data}, ensure_ascii=False)
    writer_result = await agents["writer"].run(task=writer_input)
    story = extract_content(writer_result.messages)
    print(f"âœ… åˆç¨¿å®Œæˆ: {len(story)} å­—ç¬¦")

    # Step 3: å¤šè½®ä¿®è®¢
    max_rounds = 3
    for round_num in range(max_rounds):
        print(f"\nğŸ”„ Step 3.{round_num + 1}: ç¬¬ {round_num + 1} è½®æ ¡éªŒä¸ä¿®è®¢...")
        
        # å¹¶è¡Œæ ¡éªŒ
        fact_task = agents["fact_checker"].run(task=story)
        edit_task = agents["editor"].run(task=story)
        fact_result, edit_result = await asyncio.gather(fact_task, edit_task)
        
        fact_content = extract_content(fact_result.messages)
        edit_content = extract_content(edit_result.messages)
        
        fact_feedback = parse_json_response(fact_content)
        edit_feedback = parse_json_response(edit_content)
        
        # æ£€æŸ¥æ˜¯å¦é€šè¿‡
        fact_passed = fact_feedback.get("is_accurate", False) or "FACT_CHECK_PASSED" in fact_content
        edit_passed = edit_feedback.get("is_approved", False) or "EDIT_APPROVED" in edit_content
        
        if fact_passed and edit_passed:
            print(f"âœ… ç¬¬ {round_num + 1} è½®é€šè¿‡ï¼")
            break
            
        # å‡†å¤‡ä¿®è®¢è¾“å…¥
        revision_input = json.dumps({
            "research": research_data,
            "revision_feedback": {
                "fact_check": fact_feedback,
                "editor": edit_feedback
            },
            "current_story": story
        }, ensure_ascii=False, indent=2)
        
        # ä½œå®¶ä¿®è®¢
        writer_result = await agents["writer"].run(task=revision_input)
        story = extract_content(writer_result.messages)
        print(f"âœ… ä¿®è®¢å®Œæˆ: {len(story)} å­—ç¬¦")
        
        # æ˜¾ç¤ºåé¦ˆæ‘˜è¦
        fact_issues = fact_feedback.get("issues", [])
        edit_weaknesses = edit_feedback.get("weaknesses", [])
        if fact_issues or edit_weaknesses:
            print(f"ğŸ“ ä¿®è®¢è¦ç‚¹: {fact_issues[:2] + edit_weaknesses[:2]}")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    output_data = {
        "original_text": shanhai_text,
        "research": research_data,
        "final_story": story,
        "revision_rounds": round_num + 1
    }
    
    with open("shanhai_final_output.json", "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    with open("shanhai_final_story.md", "w", encoding="utf-8") as f:
        f.write(f"# ã€Šå±±æµ·ç»ã€‹ç¥è¯æ•…äº‹\n\n")
        f.write(f"## åŸæ–‡\n{shanhai_text}\n\n")
        f.write(f"## è€ƒæ®\n```json\n{json.dumps(research_data, ensure_ascii=False, indent=2)}\n```\n\n")
        f.write(f"## æ•…äº‹\n{story}\n")
    
    print(f"\nâœ… æœ€ç»ˆæ•…äº‹å·²ä¿å­˜:")
    print(f"   - JSON æ ¼å¼: shanhai_final_output.json")
    print(f"   - Markdown æ ¼å¼: shanhai_final_story.md")
    
    await model_client.close()
    print("\nğŸ‰ å±±æµ·ç»ç¼–è¾‘ç¤¾åä½œå®Œæˆï¼")

if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()
    asyncio.run(main())
