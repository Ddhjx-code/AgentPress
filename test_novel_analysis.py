#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆå°è¯´åˆ†ææµ‹è¯• - ä»…æµ‹è¯•å‰å‡ ä¸ªæ®µè½
"""
import asyncio
import sys
from pathlib import Path
import os
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from core.workflow_service import WorkflowService
from knowledge.pdf_processor import PDFProcessor
from knowledge.literary_analyzer import LiteraryAnalyzer
from knowledge.novel_knowledge_extender import NovelKnowledgeExtender

load_dotenv()  # åŠ è½½ç¯å¢ƒå˜é‡


async def test_novel_analysis():
    """æµ‹è¯•å°è¯´åˆ†æåŠŸèƒ½"""
    print("ğŸ”§ åˆå§‹åŒ–å·¥ä½œæµæœåŠ¡...")

    workflow_service = WorkflowService()

    # è·å–APIé…ç½®
    api_key = os.getenv("QWEN_API_KEY", "")
    base_url = os.getenv("BASE_URL", "https://apis.iflow.cn/v1")
    model_name = os.getenv("MODEL_NAME", "qwen3-max")

    if not api_key:
        print("âš ï¸  æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ QWEN_API_KEY è®¾ç½®")
        return

    print("âœ… APIå¯†é’¥å·²åŠ è½½")

    try:
        await workflow_service.initialize_models(
            api_key=api_key,
            base_url=base_url,
            model_name=model_name
        )
        print("âœ… å·¥ä½œæµæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ å·¥ä½œæµåˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # åˆ›å»ºä¸€ä¸ªç®€åŒ–ç‰ˆçš„æµ‹è¯•
    processor = PDFProcessor()

    # åªæå–PDFçš„å‰å‡ é¡µå†…å®¹
    pdf_data = processor.extract_pdf_content("å®‰å¾’ç”Ÿç«¥è¯é€‰.pdf")
    content = pdf_data['content']

    # æˆªå–å‰3ä¸ªå°èŠ‚/ç« èŠ‚çš„å†…å®¹ç”¨äºæµ‹è¯•ï¼ˆé¿å…å¤§é‡APIè°ƒç”¨ï¼‰
    # æ‰¾åˆ°å‰å‡ ä¸ªæ®µè½
    paragraphs = content.split('\n\n')

    # å–å‰5ä¸ªæ®µè½ç”¨äºæµ‹è¯•ï¼Œç¡®ä¿æ€»å­—ç¬¦æ•°é€‚ä¸­
    test_paragraphs = []
    total_chars = 0
    for para in paragraphs:
        para = para.strip()
        if para and len(para) > 10:  # éç©ºä¸”å¤Ÿé•¿çš„æ®µè½
            if total_chars + len(para) < 2000:  # ç´¯è®¡ä¸è¶…è¿‡2000å­—ç¬¦
                test_paragraphs.append(para)
                total_chars += len(para)
            else:
                break

    print(f"ğŸ“– æµ‹è¯•å†…å®¹æ®µè½æ•°: {len(test_paragraphs)}")
    for i, para in enumerate(test_paragraphs):
        print(f"  æ®µè½ {i+1}: {len(para)} å­—ç¬¦ - \"{para[:60]}...\"")

    print("\nğŸ”„ åˆå§‹åŒ–æµ‹è¯•åˆ†ææ¨¡å—...")

    # åˆ›å»ºä¸€ä¸ªæ‰©å±•ç®¡ç†å™¨å¹¶è¿›è¡Œç®€åŒ–æµ‹è¯•
    extender = NovelKnowledgeExtender(workflow_service)

    # æ‰‹åŠ¨åˆ›å»ºåˆ†æ®µçš„æ•°æ®ç»“æ„ï¼ˆæ¨¡æ‹ŸPDFåˆ†å‰²ç»“æœï¼‰
    test_segments = [
        {
            'id': f'test_para_{i:03d}',
            'text': para,
            'original_pos': i,
            'chapter_info': {'section_title': 'æµ‹è¯•æ®µè½'},
            'word_count': len(para),
            'is_chapter_header': False,
            'section_title': f'æµ‹è¯•æ®µè½ {i+1}',
            'original_title': 'å®‰å¾’ç”Ÿç«¥è¯é€‰-æµ‹è¯•'
        }
        for i, para in enumerate(test_paragraphs)
    ]

    print(f"ğŸ” å¼€å§‹åˆ†æ {len(test_segments)} ä¸ªæµ‹è¯•æ®µè½...")

    # åªåˆ†æå‰å‡ ä¸ªæ®µè½ï¼Œè€Œä¸æ˜¯å…¨éƒ¨521ä¸ª
    success_count = 0
    results = []

    for i, segment in enumerate(test_segments):
        print(f"  åˆ†ææ®µè½ {i+1}/{len(test_segments)}...")
        try:
            entry = await extender.literary_analyzer.analyze_paragraph(
                segment,
                'å®‰å¾’ç”Ÿç«¥è¯é€‰-æµ‹è¯•'
            )
            if entry:
                # ä¿å­˜åˆ°çŸ¥è¯†åº“
                success = await extender.km.add_entry(
                    title=entry.title,
                    content=entry.content,
                    tags=entry.tags,
                    knowledge_type=entry.knowledge_type,
                    source=entry.source
                )
                if success:
                    print(f"    âœ… æˆåŠŸåˆ›å»ºçŸ¥è¯†æ¡ç›®: {entry.knowledge_type} - {len(entry.content)} å­—ç¬¦")
                    results.append(entry)
                    success_count += 1
            else:
                print(f"    âš ï¸ æœªç”ŸæˆçŸ¥è¯†æ¡ç›®ï¼Œå¯èƒ½æ­¤æ®µä¸éœ€è¦åˆ†æ")
        except Exception as e:
            print(f"    âŒ åˆ†æå¤±è´¥: {str(e)}")
            continue

    print(f"\nğŸ¯ æµ‹è¯•ç»“æœ:")
    print(f"   æˆåŠŸåˆ†æ: {success_count}/{len(test_segments)} ä¸ªæ®µè½")
    print(f"   æ–°å¢çŸ¥è¯†æ¡ç›®: {len(results)} ä¸ª")

    if results:
        print(f"\nğŸ“ æ–°å¢çŸ¥è¯†ç¤ºä¾‹:")
        for i, entry in enumerate(results):
            print(f"   {i+1}. ç±»å‹: {entry.knowledge_type}")
            print(f"      æ ‡é¢˜: {entry.title[:50]}...")
            print(f"      æ ‡ç­¾: {entry.tags}")
            print(f"      å†…å®¹é•¿åº¦: {len(entry.content)} å­—ç¬¦")
            print(f"      æ‘˜å½•: {entry.content[:100]}...")
            print()

    # æ˜¾ç¤ºæœ€æ–°çŸ¥è¯†åº“ä¿¡æ¯
    all_entries = await extender.km.get_all_entries()
    print(f"ğŸ“Š æœ€æ–°çŸ¥è¯†åº“ç»Ÿè®¡: {len(all_entries)} ä¸ªæ¡ç›®")

    type_stats = {}
    for entry in all_entries:
        k_type = entry.knowledge_type
        type_stats[k_type] = type_stats.get(k_type, 0) + 1

    print(f"   æŒ‰ç±»å‹ç»Ÿè®¡: {type_stats}")

    # æ£€æŸ¥æ¡ç›®å¤§å°
    if all_entries:
        sizes = [len(entry.content) for entry in all_entries]
        avg_size = sum(sizes) / len(sizes)
        max_size = max(sizes)
        print(f"   æ¡ç›®å¤§å° - å¹³å‡: {avg_size:.0f}, æœ€å¤§: {max_size} å­—ç¬¦")


def sync_main():
    """åŒæ­¥å…¥å£ç‚¹"""
    import asyncio

    if sys.platform.startswith("win"):
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

    asyncio.run(test_novel_analysis())


if __name__ == "__main__":
    sync_main()